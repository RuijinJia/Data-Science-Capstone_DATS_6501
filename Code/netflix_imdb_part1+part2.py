# -*- coding: utf-8 -*-
"""netflix_IMDB_Part1+PArt2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IRR8cnUTKu5Ll73DwOeaVMVgR4SfYfZg
"""

# import warnings
# from google.colab import drive
# warnings.filterwarnings('ignore')
# drive.mount('/content/drive')

# abspath='/content/drive/MyDrive/capstone-netflix/'

# Commented out IPython magic to ensure Python compatibility.
#%% [code]
import numpy as np
import pandas as pd
import re
import matplotlib.pyplot as plt
import seaborn as sns
# !pip install pycountry_convert
from pycountry_convert import country_alpha2_to_continent_code, country_name_to_country_alpha2
import matplotlib
import matplotlib.pyplot as plt
# %matplotlib inline
from scipy.stats import norm
import nltk 
from nltk.corpus import stopwords
nltk.download('stopwords')
from nltk.tokenize import word_tokenize
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import gensim
from gensim.utils import simple_preprocess
import gensim.corpora as corpora
from pprint import pprint
from gensim.models import CoherenceModel
import spacy
from wordcloud import WordCloud
import itertools
import operator

ntf = pd.read_csv('netflix_titles.csv')
ntf.rename(columns={'release_year':'year'},inplace=True)
ntf.head()

IMDb_movie = pd.read_csv('IMDb movies.csv')
IMDb_movie.head()

IMDb_names = pd.read_csv('IMDb names.csv')
IMDb_names.head()

IMDb_ratings = pd.read_csv('IMDb ratings.csv')
IMDb_ratings.head()

IMDb_titles = pd.read_csv('IMDb title_principals.csv')
IMDb_titles.head()

IMDb = pd.merge(IMDb_movie,IMDb_ratings,on ='imdb_title_id')
IMDb = pd.merge(IMDb,IMDb_titles,on ='imdb_title_id')
IMDb = pd.merge(IMDb,IMDb_names,on ='imdb_name_id')

IMDb.head()

# IMDb[IMDb['imdb_title_id']=='tt0000009']

# IMDb.columns

IMDb_gender_age = pd.concat([IMDb.loc[:,'weighted_average_vote'],IMDb.loc[:, 'allgenders_0age_avg_vote':'females_45age_votes']],axis=1)
IMDb_gender_age = pd.concat([IMDb.loc[:,['title','year']],IMDb_gender_age],axis=1)
IMDb_gender_age.head()

IMDb_gender_age.describe()

"""IMDB weighted average:

w = v*R/(v+m) + m*C/(v+m)

where

v = number of votes on the movie

m = minimum number of votes to be in the top 50 (currently 1000 votes)

R = average/mean rating for the movie

C = mean rating across whole dataset
"""

def IMDB_WA(num, average, r):
  if r[num] != 0.0:
    results = ((r[num]/(r[num]+1000))*r[average]) + ((1000/(r[num]+1000))*5.9)
  else:
    results = 0.0
  return results

wa_allgenders_0age = []
wa_allgenders_18age = []
wa_allgenders_30age = []
wa_allgenders_45age = []
wa_males_allages = []
wa_males_0age = []
wa_males_18age = []
wa_males_30age = []
wa_males_45age = []
wa_females_allages = []
wa_females_0age = []
wa_females_18age = []
wa_females_30age = []
wa_females_45age = []

for i in range(IMDb.shape[0]):
    wa_allgenders_0age.append(IMDB_WA('allgenders_0age_votes', 'allgenders_0age_avg_vote', IMDb_gender_age.iloc[i]))
    wa_allgenders_18age.append(IMDB_WA('allgenders_18age_votes', 'allgenders_18age_avg_vote', IMDb_gender_age.iloc[i]))
    wa_allgenders_30age.append(IMDB_WA('allgenders_30age_votes', 'allgenders_30age_avg_vote', IMDb_gender_age.iloc[i]))
    wa_allgenders_45age.append(IMDB_WA('allgenders_45age_votes', 'allgenders_45age_avg_vote', IMDb_gender_age.iloc[i]))
    wa_males_allages.append(IMDB_WA('males_allages_votes', 'males_allages_avg_vote', IMDb_gender_age.iloc[i]))
    wa_males_0age.append(IMDB_WA('males_0age_votes', 'males_0age_avg_vote', IMDb_gender_age.iloc[i]))
    wa_males_18age.append(IMDB_WA('males_18age_votes', 'males_18age_avg_vote', IMDb_gender_age.iloc[i]))
    wa_males_30age.append(IMDB_WA('males_30age_votes', 'males_30age_avg_vote', IMDb_gender_age.iloc[i]))
    wa_males_45age.append(IMDB_WA('males_45age_votes', 'males_45age_avg_vote', IMDb_gender_age.iloc[i]))
    wa_females_allages.append(IMDB_WA('females_allages_votes', 'females_allages_avg_vote', IMDb_gender_age.iloc[i]))
    wa_females_0age.append(IMDB_WA('females_0age_votes', 'females_0age_avg_vote', IMDb_gender_age.iloc[i]))
    wa_females_18age.append(IMDB_WA('females_18age_votes', 'females_18age_avg_vote', IMDb_gender_age.iloc[i]))
    wa_females_30age.append(IMDB_WA('females_30age_votes', 'females_30age_avg_vote', IMDb_gender_age.iloc[i]))
    wa_females_45age.append(IMDB_WA('females_45age_votes', 'females_45age_avg_vote', IMDb_gender_age.iloc[i]))

IMDb_gender_age['weighted_allgenders_0age'] = wa_allgenders_0age
IMDb_gender_age['weighted_allgenders_18age'] = wa_allgenders_18age
IMDb_gender_age['weighted_allgenders_30age'] = wa_allgenders_30age
IMDb_gender_age['weighted_allgenders_45age'] = wa_allgenders_45age
IMDb_gender_age['weighted_males_allages'] = wa_males_allages
IMDb_gender_age['weighted_males_0age'] = wa_males_0age
IMDb_gender_age['weighted_males_18age'] = wa_males_18age
IMDb_gender_age['weighted_males_30age'] = wa_males_30age
IMDb_gender_age['weighted_males_45age'] = wa_males_45age
IMDb_gender_age['weighted_females_allages'] = wa_females_allages
IMDb_gender_age['weighted_females_0age'] = wa_females_0age
IMDb_gender_age['weighted_females_18age'] = wa_females_18age
IMDb_gender_age['weighted_females_30age'] = wa_females_30age
IMDb_gender_age['weighted_females_45age'] = wa_females_45age

IMDb_gender_age.head()

ntf.head()

"""Select Top10 Movie by gender and age group"""

## netflix+IMDb
ntf_IMDb = ntf.merge(IMDb_gender_age,how='inner',on=['title','year'])

ntf_IMDb.head()

a = ntf_IMDb.loc[:,['title','weighted_average_vote']]
b = a.drop_duplicates().reset_index(drop=True)
b = b.sort_values('weighted_average_vote',ascending= False)
c = b.head(15).reset_index(drop=True)
b['weighted_average_vote'][c.index[-1]]

def Top_Gender_Age(Group, number, Age):
  df = ntf_IMDb.loc[:,['title',Group]]
  df = df.iloc[df['title'].drop_duplicates().index].reset_index(drop=True)
  df = df.sort_values(Group, ascending = False)
  df_top = df.head(number).reset_index(drop=True)
  sns.set(rc={'figure.figsize':(12,10)})
  ax = sns.barplot(x=df_top[Group],y=df_top['title'], data=df_top, palette=("crest_d"))
  ax.set(xlabel='Rating', ylabel='Movie Title')
  ax.set_title('Best Rated Movie among Users Aged {}'.format(Age), size= 20)
  for index in range(df_top.index[-1]+1):
    ax.text(df_top[Group][index], index, round(df_top[Group][index],1), color = 'black')
  return

Top_Gender_Age('males_30age_avg_vote',10,'30-45(Males)')

Top_Gender_Age('females_30age_avg_vote', 10,'30-45(Females)')

Top_Gender_Age('weighted_allgenders_18age',10,'18-30')

"""Top Movies by Genre"""

def Top_Genre(Genre, number):
  df = ntf_IMDb.loc[:,['title','weighted_average_vote','listed_in']]
  df = df.iloc[df['title'].drop_duplicates().index].reset_index(drop=True)
  df = df.sort_values('weighted_average_vote', ascending = False)
  index = []
  for i in range(df.shape[0]):
    if Genre in df['listed_in'][i]:
      index.append(i)
  Genre_top = df.iloc[index].reset_index(drop=True)
  Genre_top = Genre_top.head(number).reset_index(drop=True)
  sns.set(rc={'figure.figsize':(12,10)})
  ax = sns.barplot(x=Genre_top['weighted_average_vote'],y=Genre_top['title'], data=Genre_top, palette=("crest_d"))
  ax.set(xlabel='Rating', ylabel='Movie Title')
  ax.set_title('Best {} on IMDb'.format(Genre), size= 20)
  for index in range(Genre_top.index[-1]+1):
    ax.text(Genre_top['weighted_average_vote'][index], index, round(Genre_top['weighted_average_vote'][index],1), color = 'black')
  return

Top_Genre('Horror Movies',10)

Top_Genre('Comedies',10)

Top_Genre('Dramas',10)

Top_Genre('Romantic Movies',10)

"""Distribution of Different Genre by Rating"""

genre = []
for i in range(ntf_IMDb.shape[0]):
  genre.append(re.split(r', \s*', ntf_IMDb['listed_in'][i])[0])
ntf_IMDb['Genre'] = genre
sns.displot(ntf_IMDb, x="weighted_average_vote", hue="Genre")

"""Rating by Countries"""

ntf_IMDb_country = ntf_IMDb.iloc[ntf_IMDb['country'].dropna().index,:]
ntf_IMDb_country = ntf_IMDb_country.reset_index(drop=True)

Country = []
for i in range(ntf_IMDb_country.shape[0]):
  Country.append(re.split(r', \s*', ntf_IMDb_country['country'][i])[0])

ntf_IMDb_country['Country'] = Country
ntf_IMDb_country.head()

from sklearn.preprocessing import MultiLabelBinarizer 

def relation_heatmap(df, title):
    df['genre'] = df['listed_in'].apply(lambda x :  x.replace(' ,',',').replace(', ',',').split(',')) 
    Types = []
    for i in df['genre']: Types += i
    Types = set(Types)
    print("There are {} types in the Netflix {} Dataset".format(len(Types),title))    
    test = df['genre']
    mlb = MultiLabelBinarizer()
    res = pd.DataFrame(mlb.fit_transform(test), columns=mlb.classes_, index=test.index)
    corr = res.corr()
    mask = np.zeros_like(corr, dtype=np.bool)
    mask[np.triu_indices_from(mask)] = True
    fig, ax = plt.subplots(figsize=(10, 7))
    pl = sns.heatmap(corr, mask=mask, cmap= "coolwarm", vmax=.5, vmin=-.5, center=0, square=True, linewidths=.7,
                     cbar_kws={"shrink": 0.6})
    
    plt.show()
relation_heatmap(ntf_IMDb, 'Genre')

rating_order_movie =  ['G', 'TV-Y', 'TV-G', 'PG', 'TV-Y7', 'TV-Y7-FV', 'TV-PG', 'PG-13', 'TV-14', 'R', 'NC-17', 'TV-MA']

movie_agerating = ntf_IMDb['rating'].value_counts(dropna=True)

def rating_barplot(data, title, height, h_lim=None):
    fig, ax = plt.subplots(1,1, figsize=(15, 7))
    if h_lim :
        ax.set_ylim(0, h_lim)
    ax.bar(data.index, data,  color="#d0d0d0", width=0.6, edgecolor='black')

    color =  ['green',  'blue',  'orange',  'red']
    span_range = [[0, 2], [3,  6], [7, 8], [9, 11]]

    for idx, sub_title in enumerate(['Little Kids', 'Older Kids', 'Teens', 'Mature']):
        ax.annotate(sub_title,
                    xy=(sum(span_range[idx])/2 ,height),
                    xytext=(0,0), textcoords='offset points',
                    va="center", ha="center",
                    color="w", fontsize=16, fontweight='bold',
                    bbox=dict(boxstyle='round4', pad=0.4, color=color[idx], alpha=0.6))
        ax.axvspan(span_range[idx][0]-0.4,span_range[idx][1]+0.4,  color=color[idx], alpha=0.1)

    ax.set_title(f'Distribution of {title} Rating', fontsize=20, fontweight='bold', position=(0.5, 1.0+0.03))
    plt.show()

rating_barplot(movie_agerating,'Genre', 1200)

ntf_IMDb_country = ntf_IMDb_country.loc[:,['type','Country','weighted_average_vote','year']]
ntf_IMDb_country['Country'].value_counts()
ntf_IMDb_India = ntf_IMDb_country[ntf_IMDb_country.Country=='India']
India_rating = ntf_IMDb_India.groupby('year')['weighted_average_vote'].mean()

plt.figure(figsize=(16, 8))
plt.plot(India_rating.index, India_rating)
plt.title('Indian Movie Rating Over Time')
plt.xlabel("Year")
plt.ylabel("Average Rating")
# plt.figure(figsize=(16, 8))
# sns.lineplot(data=India_rating, x=India_rating.index, y=India_rating)
# plt.title('Indian Movie Rating Over Time')
# plt.xlabel("Year")
# plt.ylabel("Average Rating")

ntf_IMDb_US = ntf_IMDb_country[ntf_IMDb_country.Country=='United States']
US_rating = ntf_IMDb_US.groupby('year')['weighted_average_vote'].mean()
plt.figure(figsize=(16, 8))
sns.lineplot(data=US_rating, x=US_rating.index, y=US_rating)
plt.title('US Movie Rating Over Time')
plt.xlabel("Year")
plt.ylabel("Average Rating")

ntf_IMDb_UK = ntf_IMDb_country[ntf_IMDb_country.Country=='United Kingdom']
UK_rating = ntf_IMDb_UK.groupby('year')['weighted_average_vote'].mean()
plt.figure(figsize=(16, 8))
sns.lineplot(data=UK_rating, x=UK_rating.index, y=UK_rating)
plt.title('United Kingdom Movie Rating Over Time')
plt.xlabel("Year")
plt.ylabel("Average Rating")

base_color = sns.color_palette()[3]
plt.figure(figsize=(16, 8))
sns.countplot(y = 'Country',color= base_color, data = ntf_IMDb_country, order = ntf_IMDb_country.Country.value_counts().head(10).index)
plt.title("Top 10 Countries in film Production ", fontsize = 20);

# ntf_IMDb[ntf_IMDb["type"] == "Movie"].groupby('country').count().sort_values('title', ascending=False).reset_index().country.head(10)
# # ntf_IMDb[ntf_IMDb["type"] == "Movie"].groupby('country').count().sort_values('title', ascending=False).reset_index().title.head(10)
# bar_country_movie = ntf_IMDb_country[ntf_IMDb_country["type"] == "Movie"].groupby('Country').count().sort_values('type',ascending=False).head(10)
# bar_country_tv = ntf_IMDb_country[ntf_IMDb_country["type"] == "TV Show"].groupby('Country').count().sort_values('type',ascending=False).head(10)

# plt.figure(figsize=(16, 8))
# plt.fontsize = 20
# plt.xticks(rotation=75)
# plt.title(label='Number of Titles Available by Country (Movies in Blue)')
# plt.xlabel("Country")
# plt.ylabel("Number of Titles")
# sns.barplot(bar_country_movie.index, bar_country_movie['type'], color="Blue");
# sns.barplot(bar_country_tv.index, bar_country_tv['type'], color="Red");
# plt.legend({"Movies":"Blue", "TV Shows": "Red"});

plt.figure(figsize=(16, 8))
sns.distplot(ntf_IMDb['duration'].str.extract('(\d+)'), fit=norm, kde=False, color=['red'])
plt.title("Normal distribution of movies' duration", fontweight="bold")
plt.show()

"""News and TV Shows on Netflix"""

ntf_movie = ntf[ntf['type']=='Movie']
ntf_tv = ntf[ntf['type']=='TV Show']

ntf2 = ntf.loc[:,['year','type']]
ntf_year_type = ntf2.groupby(['year','type']).size().reset_index(name='counts')
ntf_year_type = ntf_year_type.sort_values(by='year',ascending=False).reset_index()
ntf_year_type = ntf_year_type.drop(columns=['index'])
print(ntf_year_type)

x_year = np.sort(ntf['year'].value_counts().index)[-11:-1]
y_movie = ntf_movie.groupby('year').size()[-11:-1]
y_tv = ntf_tv.groupby('year').size()[-11:-1]
x1 = np.arange(len(x_year))

fig, ax = plt.subplots()
ax.bar(x1, y_movie, width=0.35, label='Movie')
ax.bar(x1+0.35, y_tv, width=0.35, label='TV Show')

ax.set_ylabel('Number')
ax.set_xlabel('Year')
ax.set_title('Movies vs. TV Shows over 10 Years')
ax.set_xticks(x1)
ax.set_xticklabels(x_year)
ax.legend()

plt.show()

ntf_country = ntf.iloc[ntf['country'].dropna().index,:]
ntf_country = ntf_country.reset_index(drop=True)

Country = []
for i in range(ntf_country.shape[0]):
  Country.append(re.split(r', \s*', ntf_country['country'][i])[0])

ntf_country['Country'] = Country
ntf_country.head()

x_country = ntf_country['Country'].value_counts()[:10].index
ntf_country_movie = ntf_country[ntf_country['type']=='Movie']
ntf_country_tv = ntf_country[ntf_country['type']=='TV Show']
y_country_movie = ntf_country_movie.groupby('Country').size()[:10]
y_country_tv = ntf_country_tv.groupby('Country').size()[:10]

y_country_movie = [None]*len(x_country)
for i in range(len(x_country)):
  y_country_movie[i] = ntf_country_movie[ntf_country_movie['Country'] == x_country[i]].shape[0]
y_country_tv = [None]*len(x_country)
for i in range(len(x_country)):
  y_country_tv[i] = ntf_country_tv[ntf_country_tv['Country'] == x_country[i]].shape[0]

x2 = np.arange(len(x_country))
plt.figure(figsize=(16, 8))
plt.bar(x2, list(y_country_movie), width=0.35,label='Movie')
plt.bar(x2+0.35, list(y_country_tv), width=0.35, label='TV Show')
plt.xticks(x2, x_country)
plt.xlabel("Country")
plt.ylabel("Number")
plt.title('Movies vs. TV Shows over TOP10 Countries')
plt.legend()
plt.show()

"""Topic Modeling & Cosine Similarity"""

## I will include information of type, title, director, cast, country, listed in, description into recomendation engine
col = ['title', 'type', 'listed_in', 'director', 'cast', 'rating', 'description']
ntf['all_info'] = ntf[col].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)
ntf['all_info'] = ntf['all_info'].map(lambda x: re.sub("([^\x00-\x7F])+","", x))

## creating document term matrix
all_info = ntf['all_info']
count_vectorizer = CountVectorizer(stop_words='english') # convert all words to lowercase and remove stop words
sparse_matrix = count_vectorizer.fit_transform(all_info)
doc_term_matrix = sparse_matrix.todense()
matrix_ntf = pd.DataFrame(doc_term_matrix, columns=count_vectorizer.get_feature_names(), index=ntf.index)

similarity_scores = cosine_similarity(sparse_matrix, sparse_matrix) 
scores = pd.DataFrame(similarity_scores )

def recommend(title):
    recommended = []
    title = title.lower()
    ntf['title'] = ntf['title'].str.lower()
    index = ntf[ntf['title']==title].index[0]
    top10_list = list(scores.iloc[index].sort_values(ascending = False).iloc[1:11].index)
    for each in top10_list:
        recommended.append(ntf.iloc[each].title) 
    return recommended

recommend('Avengers: Infinity War')

"""LDA"""

## include information of title, director, cast, country, description into recomendation engine
col = ['title', 'director', 'cast', 'rating', 'description']
ntf['all_info2'] = ntf[col].apply(lambda row: ' '.join(row.values.astype(str)), axis=1)
ntf['all_info2'] = ntf['all_info2'].map(lambda x: re.sub("([^\x00-\x7F])+","", x))

def sentence(sentences):
    for item in sentences:
        yield(gensim.utils.simple_preprocess(str(item), deacc=True)) 

sentence_words = list(sentence(ntf['all_info2']))

stop_words = stopwords.words('english')
def remove_stopwords(texts):
    return [[word for word in simple_preprocess(str(doc)) if word not in stop_words] for doc in texts]
words_removestop= remove_stopwords(sentence_words)

# Build the bigram 
bigram = gensim.models.Phrases(sentence_words, min_count=5, threshold=10) # higher threshold fewer phrases.
bigram = gensim.models.phrases.Phraser(bigram)
def bigrams(input):
    return [bigram[doc] for doc in input]
word_bigrams = bigrams(words_removestop)

def lemmatization(texts, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV']):
    texts_out = []
    for sent in texts:
        doc = nlp(" ".join(sent)) 
        texts_out.append([token.lemma_ for token in doc if token.pos_ in allowed_postags])
    return texts_out
nlp = spacy.load("en_core_web_sm", disable=['parser', 'ner'])
data_lemmatization = lemmatization(word_bigrams, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])

## wordcloud
words = list(itertools.chain.from_iterable(data_lemmatization))
long_string = ','.join(word for word in words)

wordcloud = WordCloud(background_color="white", max_words=1000, contour_width=3, contour_color='steelblue', collocations = False,  random_state=1)
wordcloud.generate(long_string)
wordcloud.to_image()

dic = corpora.Dictionary(data_lemmatization) # Create Dictionary
dic.filter_extremes(no_below=2, no_above=0.9) # Filtering too little document(1) and too common document(more than 90%) 
corpus = [dic.doc2bow(i) for i in data_lemmatization] ## Term Document Frequency

# word_freq = []
# word_list = []
## visualized key words
count_dic = {}
for i in dic.values():
    count_dic[i] = words.count(i)
sorted_dic = sorted(count_dic.items(), key=operator.itemgetter(1), reverse=True)[0:10]

plt.figure(figsize=(16,6))
plt.bar(range(len(sorted_dic)), [val[1] for val in sorted_dic], align='center')
plt.xticks(range(len(sorted_dic)), [val[0] for val in sorted_dic])
plt.xticks(rotation=70)
plt.xlabel('words')
plt.ylabel('counts')
plt.title('Top 10 key words')

# setting data labels
ax = plt.gca()
for p in ax.patches:
    ax.text(p.get_x() + p.get_width()/2., p.get_height(), '%d' % int(p.get_height()), 
            fontsize=12, color='grey', ha='center', va='bottom')
    
plt.show()

def coherence(k):
  lda_model = gensim.models.LdaMulticore(corpus=corpus,id2word=dic,
                                       num_topics=k, 
                                       random_state=100,
                                       chunksize=100,
                                       passes=10,
                                       per_word_topics=True)
  coherence = CoherenceModel(model=lda_model, texts=data_lemmatization, dictionary=dic, coherence='c_v')
  coherence_lda = coherence.get_coherence()
  return coherence_lda

# find best number of topic, iterate over 1 to 30
topic_num = range(1, 40)
coherence_score = []
for num in topic_num:
    coherence_score.append(coherence(num))

coherence_frame = pd.DataFrame()
coherence_frame["topic number"] = topic_num
coherence_frame['coherence score'] = coherence_score
coherence_frame

plt.figure(figsize=(16, 8))
plt.plot(topic_num, coherence_frame['coherence score'])

plt.title("Choosing best LDA Model")
plt.xlabel("Num Topics")
plt.ylabel("Coherence Scores")
plt.show()

lda_model = gensim.models.LdaMulticore(corpus=corpus,id2word=dic,
                                       num_topics=37, 
                                       random_state=100,
                                       chunksize=100,
                                       passes=10,
                                       per_word_topics=True)

# pprint(lda_model.print_topics())
# doc_lda = lda_model[corpus]

# !pip install -U pyLDAvis
import pyLDAvis
import pyLDAvis.gensim_models as gensimvis
# import pyLDAvis.gensim
# import pickle 
# import pyLDAvis
# pyLDAvis.enable_notebook()
LDAvis_prepared = gensimvis.prepare(lda_model, corpus, dic, sort_topics=False)
LDAvis_prepared

m1 = pyLDAvis.save_html(LDAvis_prepared,'lda.html')